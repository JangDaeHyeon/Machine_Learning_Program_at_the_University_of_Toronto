{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Week 2 Reinforcement Learning - Evaluation\n",
        "\n",
        "**Email your solutions in PDF (or Python Notebook) to `matin.moezzi@mail.utoronto.ca`**\n",
        "\n",
        "**Dynamic Programming in RL:**\n",
        "1. Which RL technique uses a complete model of the environment and finds the optimal policy in a deterministic way? [c]\n",
        "\n",
        "   a) Monte Carlo Methods\n",
        "\n",
        "   b) Temporal Difference Learning\n",
        "\n",
        "   c) Dynamic Programming\n",
        "\n",
        "   d) Q-learning\n",
        "\n",
        "**Policy Iteration:**\n",
        "2. Which of the following best describes the steps of policy iteration? [c]\n",
        "\n",
        "   a) Policy Evaluation, Policy Stabilization\n",
        "\n",
        "   b) Policy Stabilization, Policy Degradation\n",
        "\n",
        "   c) Policy Evaluation, Policy Improvement\n",
        "\n",
        "   d) Policy Initialization, Policy Convergence\n",
        "\n",
        "3. In policy iteration, when can we guarantee that policy improvement stops altering the policy? [a]\n",
        "\n",
        "   a) When the state values are maximized\n",
        "\n",
        "   b) When the action values are maximized\n",
        "\n",
        "   c) When the policy achieves a local optimum\n",
        "\n",
        "   d) When two consecutive policies are identical\n",
        "\n",
        "**Value Iteration:**\n",
        "4. In value iteration, the primary difference from policy iteration is:[b]\n",
        "\n",
        "   a) It doesn't require a model of the environment.\n",
        "\n",
        "   b) It doesn't have the policy evaluation step.\n",
        "\n",
        "   c) It uses a different form of the Bellman equation.\n",
        "\n",
        "   d) It doesn't try to improve the policy.\n",
        "\n",
        "5. In value iteration, what is repeatedly updated until convergence? [b]\n",
        "\n",
        "   a) Policy\n",
        "\n",
        "   b) Value Function\n",
        "\n",
        "   c) Reward Function\n",
        "\n",
        "   d) Model of the environment\n",
        "\n",
        "**Exploration-Exploitation Trade-off:**\n",
        "6. What is a common method to address the exploration-exploitation dilemma in RL? [b]\n",
        "\n",
        "   a) Bellman Expectation Equation\n",
        "\n",
        "   b) ε-greedy policy\n",
        "\n",
        "   c) Policy Evaluation\n",
        "\n",
        "   d) Value Iteration\n",
        "\n",
        "7. Which strategy always exploits its knowledge without exploring? [b]\n",
        "\n",
        "   a) ε-greedy with ε=0.1\n",
        "\n",
        "   b) ε-greedy with ε=0\n",
        "\n",
        "   c) Softmax\n",
        "\n",
        "   d) None of them\n",
        "\n",
        "**Monte Carlo Methods:**\n",
        "8. The Monte Carlo method in RL:[b]\n",
        "\n",
        "   a) Uses the Bellman equation for updates.\n",
        "\n",
        "   b) Requires full episodes to estimate values.\n",
        "\n",
        "   c) Can update values in the middle of an episode.\n",
        "\n",
        "   d) Always provides the optimal policy after one episode.\n",
        "\n",
        "9. Which of the following best describes the convergence speed difference between Monte Carlo and Temporal Difference (TD) learning methods? [b]\n",
        "\n",
        "  a) Monte Carlo always converges faster because it uses entire episodes.\n",
        "\n",
        "  b) TD learning tends to converge faster due to its bootstrapping nature.\n",
        "\n",
        "  c) Both converge at the same speed because they rely on the same update mechanism.\n",
        "\n",
        "  d) Convergence speed primarily depends on the learning rate and not the algorithm.\n",
        "\n",
        "\n",
        "**TD Learning:**\n",
        "10. Which of the following is a TD learning method? [a]\n",
        "\n",
        "   a) Monte Carlo\n",
        "\n",
        "   b) Deep Q Networks\n",
        "\n",
        "   c) PILCO\n",
        "\n",
        "   d) Value Iteration\n",
        "\n",
        "11. Which of the following is a model-free TD learning method? [a]\n",
        "\n",
        "   a) SARSA\n",
        "\n",
        "   b) Value Iteration\n",
        "\n",
        "   c) Dynamic Programming\n",
        "\n",
        "   d) Policy Gradients\n",
        "\n",
        "**On-policy vs. Off-policy:**\n",
        "12. SARSA is an ______ method while Q-learning is an ______ method. [b]\n",
        "\n",
        "   a) On-policy, On-policy\n",
        "\n",
        "   b) On-policy, Off-policy\n",
        "\n",
        "   c) Off-policy, On-policy\n",
        "\n",
        "   d) Off-policy, Off-policy\n",
        "\n",
        "13. Which of the following is an on-policy method? [c]\n",
        "\n",
        "   a) Q-learning\n",
        "\n",
        "   b) Expected-SARSA\n",
        "\n",
        "   c) Dyna-Q\n",
        "\n",
        "   d) DQN\n",
        "\n",
        "**Policy Gradient vs. Value-based:**\n",
        "14. Which type of algorithm directly optimizes the policy? [c]\n",
        "\n",
        "   a) Value-based methods\n",
        "\n",
        "   b) Model-based methods\n",
        "\n",
        "   c) Policy gradient methods\n",
        "\n",
        "   d) Dynamic Programming\n",
        "  \n",
        "\n",
        "15. Which method directly optimizes the policy without using a value function? [c]\n",
        "\n",
        "   a) Q-learning\n",
        "\n",
        "   b) Value Iteration\n",
        "\n",
        "   c) Policy Gradients\n",
        "\n",
        "   d) DQN\n",
        "\n",
        "**Model-free vs Model-based:**\n",
        "16. Which type of learning does not require knowledge about the model of the environment? [c]\n",
        "\n",
        "   a) On-policy learning\n",
        "\n",
        "   b) Model-based learning\n",
        "\n",
        "   c) Model-free learning\n",
        "\n",
        "   d) Off-policy learning\n",
        "\n",
        "17. Which of the following methods requires a model of the environment? [b]\n",
        "\n",
        "   a) Every-visit Monte Carlo Control\n",
        "\n",
        "   b) Dynamic Programming\n",
        "\n",
        "   c) Q-learning\n",
        "\n",
        "   d) REINFORCE\n",
        "\n",
        "**Epsilon-greedy Policy:**\n",
        "18. In an ε-greedy strategy, if ε=0.1, the agent will choose the best action with a probability of: [b]\n",
        "\n",
        "   a) 10%\n",
        "\n",
        "   b) 90%\n",
        "\n",
        "   c) 50%\n",
        "\n",
        "   d) 0%\n",
        "\n",
        "19. When ε=1 in an ε-greedy policy, the agent will: [b]\n",
        "\n",
        "   a) Always exploit\n",
        "\n",
        "   b) Always explore\n",
        "\n",
        "   c) Neither explore nor exploit\n",
        "\n",
        "   d) Exploit with 0.1 probability\n",
        "\n",
        "20. Which algorithm uses the Q-value of the actual next action taken, making it susceptible to short-term variations in rewards? [c]\n",
        "\n",
        "  a) Expected-SARSA\n",
        "\n",
        "  b) Q-learning\n",
        "\n",
        "  c) SARSA\n",
        "\n",
        "  d) Double Q-learning\n",
        "\n",
        "**Stochastic vs. Deterministic Environments:**\n",
        "\n",
        "21. In which environment is the next state uncertain given a state and action? [b]\n",
        "\n",
        "   a) Deterministic\n",
        "\n",
        "   b) Stochastic\n",
        "\n",
        "   c) Both\n",
        "\n",
        "   d) Neither\n",
        "\n",
        "**Discrete vs Continuous Actions:**\n",
        "22. Q-learning is typically applied to problems with: [b]\n",
        "\n",
        "   a) Continuous action spaces\n",
        "\n",
        "   b) Discrete action spaces\n",
        "\n",
        "   c) Continuous state spaces\n",
        "\n",
        "   d) Infinite state spaces\n",
        "\n",
        "23. Which algorithm is specifically designed for continuous action spaces? [c]\n",
        "\n",
        "   a) DQN\n",
        "\n",
        "   b) Q-learning\n",
        "\n",
        "   c) REINFORCE\n",
        "\n",
        "   d) Policy iteration\n",
        "\n",
        "**Temporal Difference Learning:**\n",
        "24. Which of the following updates the value function based on an estimate from the current state and reward, without waiting for the final outcome? [c]\n",
        "\n",
        "   a) Monte Carlo Methods\n",
        "\n",
        "   b) Dynamic Programming\n",
        "\n",
        "   c) TD Learning\n",
        "\n",
        "   d) Model-based RL\n",
        "\n",
        "**Convergence of Algorithms:**\n",
        "25. Which of the following guarantees convergence to the optimal policy when used with function approximation? [b]\n",
        "\n",
        "   a) Q-learning\n",
        "\n",
        "   b) SARSA\n",
        "\n",
        "   c) Neither\n",
        "\n",
        "   d) Both\n",
        "\n",
        "**Variance and Bias in RL:**\n",
        "26. Monte Carlo methods typically have: [b]\n",
        "\n",
        "   a) High bias and low variance.\n",
        "\n",
        "   b) Low bias and high variance.\n",
        "\n",
        "   c) Low bias and low variance.\n",
        "\n",
        "   d) High bias and high variance.\n",
        "\n",
        "**High Dimensionality:**\n",
        "27. Which of the following is best suited for high dimensional state spaces? [b]\n",
        "\n",
        "   a) Tabular methods\n",
        "\n",
        "   b) Deep Q-Networks\n",
        "\n",
        "   c) Classical Dynamic Programming\n",
        "\n",
        "   d) Monte Carlo with enumerative states\n",
        "\n",
        "**Sample Complexity:**\n",
        "28. Which method typically requires more samples for convergence in large state spaces? [c]\n",
        "\n",
        "   a) Q-learning\n",
        "\n",
        "   b) Dynamic Programming\n",
        "\n",
        "   c) Policy Gradient methods\n",
        "\n",
        "   d) Model-based methods\n",
        "\n",
        "**Model-free Learning:**\n",
        "29. In which scenario would model-free methods be preferable over model-based? [c]\n",
        "\n",
        "   a) When the environment model is completely known.\n",
        "\n",
        "   b) When we need to plan multiple steps ahead.\n",
        "\n",
        "   c) When the environment is hard to model or changes over time.\n",
        "\n",
        "   d) When sample efficiency is the highest priority.\n",
        "\n",
        "**Complexity of Algorithms:**\n",
        "30. What are potential sources of challenge dubbed as the \"curse of dimensionality\" in RL? [c]\n",
        "\n",
        "   a) Stochasticity in policy to be learned.\n",
        "\n",
        "   b) Exploration and exploitation dilemma.\n",
        "\n",
        "   c) High dimensionality of action space.\n",
        "\n",
        "   d) Bias in sample during training stage.\n",
        "\n",
        "\n",
        "31.  Which of the following is true about SARSA? [b]\n",
        "\n",
        "   a) It is an off-policy algorithm.\n",
        "\n",
        "   b) It learns the Q-value based on the next action that's actually taken.\n",
        "\n",
        "   c) It is used primarily for policy optimization.\n",
        "\n",
        "   d) It doesn't consider the current policy when updating.\n",
        "\n",
        "32.  In First-visit Monte Carlo, how is the value of a state estimated? [b]\n",
        "\n",
        "   a) Using the average of returns from all visits to the state.\n",
        "\n",
        "   b) Using the return from the first visit to the state in an episode.\n",
        "\n",
        "   c) Using the latest return from the state.\n",
        "\n",
        "   d) Using a weighted average of all returns.\n",
        "\n",
        "33.  Which of the following describes Every-visit Monte Carlo? [a]\n",
        "\n",
        "   a) It updates values based on every occurrence of a state in an episode.\n",
        "\n",
        "   b) It only uses the first occurrence of a state to update its value.\n",
        "\n",
        "   c) It doesn't require episodes to terminate.\n",
        "\n",
        "   d) It uses a model of the environment.\n",
        "\n",
        "34. Off-policy Monte Carlo methods require which of the following techniques to correct for the difference in the policy used to generate behavior and the policy being improved? [c]\n",
        "\n",
        "   a) ε-greedy\n",
        "\n",
        "   b) Policy iteration\n",
        "\n",
        "   c) Importance sampling\n",
        "\n",
        "   d) Every-visit Monte Carlo\n",
        "\n",
        "35.  In Generalized Policy Iteration, what two processes happen alternately? [b]\n",
        "\n",
        "   a) Policy degradation and value estimation\n",
        "\n",
        "   b) Policy evaluation and policy improvement\n",
        "\n",
        "   c) Policy stabilization and value degradation\n",
        "\n",
        "   d) Value iteration and policy iteration\n",
        "\n",
        "36. What is the primary purpose of importance sampling in RL? [b]\n",
        "\n",
        "   a) To speed up learning\n",
        "\n",
        "   b) To estimate the expectation under one distribution using samples from\n",
        "   another\n",
        "\n",
        "   c) To reduce variance in Monte Carlo methods\n",
        "\n",
        "   d) To balance exploration and exploitation\n",
        "\n",
        "37. In the Actor-Critic method, what role does the \"critic\" play? [b]\n",
        "\n",
        "   a) Selecting actions\n",
        "\n",
        "   b) Evaluating the current policy\n",
        "\n",
        "   c) Adjusting the learning rate\n",
        "\n",
        "   d) Managing memory replay\n",
        "\n",
        "39. Minimax-Q is a Q-learning adaptation for which kind of environments? [b]\n",
        "\n",
        "   a) Cooperative multi-agent\n",
        "\n",
        "   b) Competitive multi-agent\n",
        "\n",
        "   c) Single agent with a deterministic environment\n",
        "\n",
        "   d) Single agent with a stochastic environment\n",
        "\n",
        "40. What advantage does using a baseline in the REINFORCE algorithm provide? [b]\n",
        "\n",
        "   a) It makes the policy deterministic.\n",
        "\n",
        "   b) It reduces the variance of the gradient estimates.\n",
        "\n",
        "   c) It speeds up the learning process by doubling the learning rate.\n",
        "\n",
        "   d) It allows for continuous action spaces.\n",
        "\n"
      ],
      "metadata": {
        "id": "B3PailmqXFPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding Questions"
      ],
      "metadata": {
        "id": "zKIfNqYHjqQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. import part"
      ],
      "metadata": {
        "id": "tnKSYz1VByAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "LxlZAtAqByHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Q-learning\n",
        "Given a transition (s, a, s', r), update the Q-value:\n"
      ],
      "metadata": {
        "id": "pkE2CDsS_T9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning_update(Q, s, a, r, s_prime, alpha, gamma):\n",
        "  max_f = np.max(Q[s_prime, :])\n",
        "  Q[s][a] = Q[s][a] + alpha * (r + gamma * max_f) - Q[s][a]\n",
        "  return Q"
      ],
      "metadata": {
        "id": "vG2oq2V7_SrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. SARSA\n",
        "Implement the SARSA update:"
      ],
      "metadata": {
        "id": "iy9KlyciBW5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sarsa_update(Q, s, a, r, s_prime, a_prime, alpha, gamma):\n",
        "  after = Q[s_prime][a_prime]\n",
        "  Q[s][a] = Q[s][a] + alpha * (r + gamma * after - Q[s][a])\n",
        "  return Q"
      ],
      "metadata": {
        "id": "-KzKCnMc_Sxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Expected SARSA\n",
        "\n",
        "Implement the Expected SARSA update"
      ],
      "metadata": {
        "id": "5WI5MdXsB85Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def expected_sarsa_update(Q, s, a, r, s_prime, policy, alpha, gamma):\n",
        "\n",
        "  a = policy(s_prime, a_prime)\n",
        "  b = []\n",
        "\n",
        "  for a_prime in range(len(Q[s_prime])):\n",
        "    b.append(Q[s_prime][a_prime])\n",
        "\n",
        "  expected_value = np.sum([a * b])\n",
        "\n",
        "  Q[s][a] = Q[s][a] + alpha * (r + gamma * expected_value - Q[s][a])\n",
        "  return Q"
      ],
      "metadata": {
        "id": "9G9Of0_r_S3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Off-Policy SARSA:\n",
        "\n",
        "Given a behavior policy, implement off-policy SARSA:"
      ],
      "metadata": {
        "id": "7lqOBLoJCg-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def off_policy_sarsa_update(Q, s, a, r, s_prime, a_prime, target_policy, behavior_policy, alpha, gamma):\n",
        "\n",
        "  target_p = target_policy(s_prime, a_prime)\n",
        "  behavior_p = behavior_policy(s_prime, a_prime)\n",
        "\n",
        "  importance_sampling = target_p / behavior_p\n",
        "\n",
        "  Q[s][a] = Q[s][a] + alpha * (importance_sampling * (r + gamma * Q[s_prime][a_prime]) - Q[s][a])\n",
        "  return Q"
      ],
      "metadata": {
        "id": "MQE9EULh_S5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Epsilon-Greedy Policy:\n",
        "\n",
        "Implement the epsilon-greedy action selection:"
      ],
      "metadata": {
        "id": "z_r_4tPWChjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy(Q, state, epsilon):\n",
        "    if np.random.uniform(0, 1) < epsilon:\n",
        "        return _________________________________\n",
        "    else:\n",
        "        return _________________________________"
      ],
      "metadata": {
        "id": "kmrU9U3R_S7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Policy Iteration:\n",
        "\n",
        "Given the value function V and transition probabilities P, implement the policy iteration:"
      ],
      "metadata": {
        "id": "boWYogGLCiDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(P, V, policy, gamma):\n",
        "    # Policy Evaluation\n",
        "    delta = float('inf')\n",
        "    while delta > 0.01:\n",
        "        delta = 0\n",
        "        for state in P:\n",
        "            v = V[state]\n",
        "            V[state] = sum([P[state][action][next_state] * (gamma * V[next_state]) for next_state in P[state][action]])\n",
        "            delta = max(delta, abs(v - V[state]))\n",
        "\n",
        "    # Policy Improvement\n",
        "    policy_stable = True\n",
        "    for state in P:\n",
        "        old_action = policy[state]\n",
        "        policy[state] = max(P[state], key=lambda a: sum([P[state][a][next_state] * (gamma * V[next_state]) for next_state in P[state][a]]))\n",
        "        if old_action != policy[state]:\n",
        "            policy_stable = False\n",
        "\n",
        "    return V, policy, policy_stable\n"
      ],
      "metadata": {
        "id": "1jWl6dlGDZ6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Dyna-SARSA\n",
        "\n",
        "Implement SARSA version of Dyna-Q"
      ],
      "metadata": {
        "id": "u3_9AYltCy4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DynaSARSA:\n",
        "    def __init__(self, alpha=0.1, gamma=0.95, epsilon=0.1, planning_n=5):\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.planning_n = planning_n\n",
        "\n",
        "        self.Q = defaultdict(float)\n",
        "        self.model = defaultdict(tuple)\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, next_action):\n",
        "\n",
        "        self.Q[(state, action)] += __________________________\n",
        "\n",
        "        # Update the model\n",
        "        self.model[(state, action)] = (reward, next_state)\n",
        "\n",
        "        # Plan using the model\n",
        "        for _ in range(self.planning_n):\n",
        "            state_sample, action_sample = random.choice(list(self.model.keys()))\n",
        "            reward_sample, next_state_sample = self.model[(state_sample, action_sample)]\n",
        "\n",
        "            self.Q[(state_sample, action_sample)] += __________________"
      ],
      "metadata": {
        "id": "PRrlpkPECzGM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}